{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-z-5svDfdh1"
      },
      "source": [
        "!pip install xlsxwriter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, MaxPooling1D, Conv1D, Flatten, BatchNormalization, Activation, LSTM\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow.keras as keras\n",
        "leaky_relu = tf.nn.leaky_relu\n",
        "\n",
        "\n",
        "def CNN_3l_model(learn_rate =.001, activation=\"relu\", init='glorot_uniform', kernel_regularizer=\"l1\", dropout=0, input_shape= (None,61)):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(280, 8, padding = \"same\", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))  # input_dim=(20, )\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(128, 8, padding = \"same\", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(64, 8, padding = \"same\", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(lr = learn_rate)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def CNN_2l_model(learn_rate =.001, activation=\"relu\", init='glorot_uniform', kernel_regularizer=\"l1\", dropout=0, input_shape= (None,61)):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(128, 8, padding = \"same\", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(64, 8, padding = \"same\", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(lr = learn_rate)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def CNN_1l_model(learn_rate =.001, activation=\"relu\", init='glorot_uniform', kernel_regularizer=\"l1\", dropout=0, input_shape= (None,61)):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv1D(32, 8, padding = \"same\", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(lr = learn_rate)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "function_list = [CNN_1l_model, CNN_2l_model, CNN_3l_model]\n",
        "\n",
        "import warnings\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "\n",
        "\n",
        "def namestr(obj, namespace):\n",
        "    return [name for name in namespace if namespace[name] is obj]\n",
        "\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "'''\n",
        "Data Pull\n",
        "Here I pull the train test data from excel files.\n",
        "'''\n",
        "\n",
        "File = \"Data/S_10pData_withNoise_1.63sec\"\n",
        "\n",
        "File_types = [\"GFCC\", \"LFCC\",\"BFCC\", \"NGCC\", \"LPC\", \"RPLP\", \"chromashift\", \"melspect\", \"chromaCqt\", \"RMS\", \"specCont\", \"tonnetz\", \"MFCC\", \"delta\", \"deltadelta\"]\n",
        "for item in File_types:\n",
        "    File_Name = File + \"_\" + item + \".xlsx\"\n",
        "\n",
        "    df = pd.read_excel(File_Name, \"Sheet1\", header=0)\n",
        "\n",
        "    print(File_Name)\n",
        "\n",
        "    people = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "    sounds = [1, 2, 3, 4, 5, 6]\n",
        "    scores_df = pd.DataFrame(None, columns=[\"Target ID\", \"Sound\", \"Model\", \"TP\", \"FN\", \"FP\", \"TN\", \"best params\"])\n",
        "    validation_df = pd.DataFrame(None, columns=[\"Target ID\", \"Sound\", \"Model\", \"means\", \"stds\", \"params\"])\n",
        "    feature_count = 20\n",
        "\n",
        "    df = df.sample(frac=1)\n",
        "\n",
        "    best_model = None\n",
        "    best_model_performance = 0\n",
        "    best_model_summary = None\n",
        "\n",
        "    for column in df.columns:\n",
        "        if ((column != \"Person ID\") & (column != \"Sound ID\")):\n",
        "            df[column] = df[column] / df[column].max()\n",
        "\n",
        "    pd.options.mode.use_inf_as_na = True\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    non_null_column = df.isnull().sum()[df.isnull().sum() == 0].index\n",
        "    df = df[non_null_column]\n",
        "\n",
        "    for person in people:\n",
        "        validUser = person\n",
        "        for sound in sounds:\n",
        "\n",
        "            k_values = [30]\n",
        "\n",
        "            for k_value in k_values:\n",
        "                print(\"validUser is \" + str(validUser))\n",
        "                print(\"sound left out\" + str(sound))\n",
        "                print(\"K features\" + str(k_value))\n",
        "                traindf = df.loc[(df[\"Person ID\"] == validUser) & (df[\"Sound ID\"] != sound)]\n",
        "                for person in people:\n",
        "                    if (person != validUser):\n",
        "                        traindf = pd.concat(\n",
        "                            [traindf, df.loc[(df[\"Person ID\"] == person) & (df[\"Sound ID\"] != sound)][:int((len(\n",
        "                                df.loc[(df[\"Person ID\"] == validUser) & (df[\"Sound ID\"] != sound)]) // (len(\n",
        "                                people) - 1))//10*10)]])\n",
        "\n",
        "                testdf = df.loc[(df[\"Person ID\"] == validUser) & (df[\"Sound ID\"] == sound)]\n",
        "                for person in people:\n",
        "                    if (person != validUser):\n",
        "                        testdf = pd.concat(\n",
        "                            [testdf, df.loc[(df[\"Person ID\"] == person) & (df[\"Sound ID\"] == sound)][:int((len(\n",
        "                                df.loc[(df[\"Person ID\"] == validUser) & (df[\"Sound ID\"] == sound)]) // (len(\n",
        "                                people) - 1))//10*10)]])\n",
        "\n",
        "                traindf = traindf.sample(frac=1)\n",
        "                testdf = testdf.sample(frac=1)\n",
        "\n",
        "                X_train = traindf.drop(columns=[\"Person ID\", \"Sound ID\"])\n",
        "                X_test = testdf.drop(columns=[\"Person ID\", \"Sound ID\"])\n",
        "\n",
        "                Y_train = traindf[\"Person ID\"]\n",
        "                Y_test = testdf[\"Person ID\"]\n",
        "                for person in people:\n",
        "                    if (person != validUser):\n",
        "                        Y_train = Y_train.replace({person: 0})\n",
        "                        Y_test = Y_test.replace({person: 0})\n",
        "\n",
        "                for person in people:\n",
        "                    if (person == validUser):\n",
        "                        Y_train = Y_train.replace({person: 1})\n",
        "                        Y_test = Y_test.replace({person: 1})\n",
        "\n",
        "                print(\"Training Size: \", len(X_train), \" balance of: \", Y_train.sum(), \":\",\n",
        "                      len(Y_train) - Y_train.sum(), \\\n",
        "                      \"Training Size: \", len(X_test), \" balance of : \", Y_test.sum(), \":\", len(Y_test) - Y_test.sum())\n",
        "\n",
        "                print(\"shapes\", X_train.shape, Y_train.shape)\n",
        "                X_train = X_train.to_numpy().reshape(int(X_train.shape[0]/10), 10, X_train.shape[1])\n",
        "                Y_train = Y_train.to_numpy().reshape(int(Y_train.shape[0]/10), 10)\n",
        "                X_test = X_test.to_numpy().reshape(int(X_test.shape[0]/10), 10, X_test.shape[1])\n",
        "                Y_test = Y_test.to_numpy().reshape(int(Y_test.shape[0]/10), 10)\n",
        "\n",
        "                print(\"final\", X_train.shape, Y_train.shape)\n",
        "                for function in function_list:\n",
        "                  model = KerasClassifier(build_fn=function, verbose=0)\n",
        "                  epochs = [50, 100, 150, 200]\n",
        "                  batches = [25, 50, 75, 100]\n",
        "                  param_grid = dict(epochs=epochs, batch_size=batches)\n",
        "                  grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "                  grid_result = grid.fit(X_train, Y_train)\n",
        "                  # summarize results\n",
        "                  # print(\"Training: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "                  means = grid_result.cv_results_['mean_test_score']\n",
        "                  stds = grid_result.cv_results_['std_test_score']\n",
        "                  params = grid_result.cv_results_['params']\n",
        "                  for mean, stdev, param in zip(means, stds, params):\n",
        "                      print(\"Trainning \" + str(validUser) + \" \" + str(sound) + \" %f (%f) with: %r\" % (mean, stdev, param))\n",
        "                      validation_info = [validUser, sound, \"CNN (Adam)\", means, stds, params]\n",
        "                      validation_df = validation_df.append(pd.Series(validation_info, index=validation_df.columns),\n",
        "                                                          ignore_index=True)\n",
        "\n",
        "                  pred_keras = grid_result.predict(X_test)\n",
        "                  matrix = confusion_matrix(Y_test.values.ravel(), pred_keras)\n",
        "                  TP, FN, FP, TN = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]\n",
        "                  score_info = [validUser, sound, \"RNN\", TP, FN, FP, TN, grid_result.best_params_]\n",
        "                  scores_df = scores_df.append(pd.Series(score_info, index=scores_df.columns), ignore_index=True)\n",
        "                  print(\"Testing \" + str(validUser) + \" \" + str(sound) + \" \" + str(TP) + \" \" + str(FN) + \" \" + str(\n",
        "                      FP) + \" \" + str(TN))\n",
        "                  print(\"Testing \" + str(validUser) + \" \" + str(sound) + \" \", grid_result.best_params_)\n",
        "\n",
        "    print(\"breakpoint save for \", item)\n",
        "    writer = pd.ExcelWriter(\"Kera_Results_for_RNN_\" + item + \"_11_19.xlsx\", engine='xlsxwriter')\n",
        "    scores_df.to_excel(writer, sheet_name='Sheet1')\n",
        "    validation_df.to_excel(writer, sheet_name=\"sheet2\")\n",
        "    writer.save()\n",
        "    print(scores_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}